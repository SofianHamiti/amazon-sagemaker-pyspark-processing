{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data with PySpark running in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstrates how to prepare a dataset with PySpark, both locally and using SageMaker Processing jobs.\n",
    "\n",
    "Amazon SageMaker allows you to run steps for data pre- or post-processing, feature engineering, data validation, or model evaluation workloads using its managed infrastructure. This reduces the overhead in compute capacity and environments management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/manual.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: you can develop and debug with Spark locally using sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "df = spark.read.csv('data/netflix_titles.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO PROCESSING HERE\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE OUTPUT FILES\n",
    "df.write.save('data/output', format='csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: when ready, you can scale Spark jobs with SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SageMaker Python sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker==2.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() # we are using the notebook instance role for processing in this example\n",
    "bucket = sagemaker_session.default_bucket() # you can specify a bucket name here\n",
    "prefix = 'spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push data to S3 from the Notebook Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now push this dataset in the default S3 Bucket from our SageMaker Notebook instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input = sagemaker_session.upload_data(\n",
    "    path='data/netflix_titles.csv', \n",
    "    bucket=bucket, \n",
    "    key_prefix=f'{prefix}/input'\n",
    ")\n",
    "print(s3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch SageMaker Processing job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/processing_spark.jpeg\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_processor = PySparkProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    base_job_name='spark-processing',\n",
    "    framework_version='2.4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output=f's3://{bucket}/{prefix}/output'\n",
    "s3_spark_event_logs=f's3://{bucket}/{prefix}/spark_event_logs'\n",
    "\n",
    "pyspark_processor.run(\n",
    "    submit_app='code/prepare_data.py',\n",
    "    arguments= [\n",
    "        f'--s3_input={s3_input}',\n",
    "        f'--s3_output={s3_output}'\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=s3_spark_event_logs,\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: using the Spark History Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While script is running, or after script has run, you can view spark UI by running history server locally or in the notebook. By default, the s3 URI you provided in previous run() method will be used as spark event source, but you can also specify a different URI. Last but not the least, you can terminate the history server with terminate_history_server(). Note that only one history server process will be running at a time. [See here](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#spark-history-server) for more details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
